{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import botocore\n",
    "import boto3\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import pandas as pd\n",
    "\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Initializations\n",
    "# Get values from Environment Variable\n",
    "filepath_metadata = os.environ['FILEPATH_METADATA']\n",
    "SAMPLING_FREQUENCY = os.environ['SAMPLING_FREQUENCY']\n",
    "DB_HOST = os.environ['DB_HOST']\n",
    "DB_PORT = os.environ['DB_PORT']\n",
    "DB_USER = os.environ['DB_USER']\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "DB_PASSWORD = os.environ['DB_PASSWORD']\n",
    "DEBUG = os.environ['DEBUG'] # boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_columnnames = ['datetime', 'station', 'interval_length', 'sp04', 'sp10', 'gu04', 'gu10', 'dn04', 'dn10', 'sg04', 'sg10', 'at02', 'at10', 'delt', 'rh02', 'rh10', 'pres', 'rain', 'swin', 'swout', 'iwin', 'iwout', 'grad', 'nrad']\n",
    "aqm_columnnames = ['datetime', 'station', 'interval_length', 'pm2_5', 'pm10', 'pmcrs', 'so2', 'co', 'no', 'no2', 'nox', 'o3']\n",
    "column_agg_mapping = {\n",
    "    'sp04': 'mean', 'sp10': 'mean', 'gu04': 'mean', 'gu10': 'mean', \n",
    "    'dn04': 'mean', 'dn10': 'mean', 'sg04': 'mean', 'sg10': 'mean',\n",
    "    'at02': 'mean', 'at10': 'mean', 'delt': 'mean', 'rh02': 'mean',\n",
    "    'rh10': 'mean', 'pres': 'mean', 'rain': 'sum', 'swin': 'mean',\n",
    "    'swout': 'mean', 'lwin': 'mean', 'lwout': 'mean', 'grad': 'mean', \n",
    "    'nrad': 'mean', 'pm2_5': 'mean', 'pm10': 'mean', 'pmcrs': 'mean',\n",
    "    'so2': 'mean', 'co': 'mean', 'no': 'mean', 'no2': 'mean',\n",
    "    'nox': 'mean', 'o3': 'mean'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "# Set logging level according to the DEBUG boolean value\n",
    "if DEBUG:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# Database connection parameters\n",
    "params_dict = {\n",
    "    \"host\": DB_HOST,\n",
    "    \"port\": DB_PORT,\n",
    "    \"database\": DB_NAME,\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASSWORD\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(filepath):\n",
    "    \"\"\"\n",
    "    returns metadata \n",
    "    :params: a valid path to metadta (JSON)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except IOError:\n",
    "        logger.error(\"An error has occurred. No such file found!\")\n",
    "        if DEBUG:\n",
    "            raise\n",
    "def get_translation_from_filename(filename, metadata):\n",
    "    \"\"\"\n",
    "    returns standard column names mapping of a file.\n",
    "    \"\"\"\n",
    "    if isinstance(metadata, dict):\n",
    "        if 'translations' in metadata:\n",
    "            translations = metadata['translations']\n",
    "\n",
    "            for i in range(len(translations)):\n",
    "                if translations[i]['filename'].lower() == filename.lower():\n",
    "                    return translations[i]['columnnames']\n",
    "    else:\n",
    "        logger.error(\"Metadata is not Dictionary Object.\")\n",
    "\n",
    "def get_metadata_from_filename(filename, metadata):\n",
    "    \"\"\"\n",
    "    returns entire metadata of a file.\n",
    "    \"\"\"\n",
    "    if isinstance(metadata, dict):\n",
    "        if 'translations' in metadata:\n",
    "            translations = metadata['translations']\n",
    "\n",
    "            for i in range(len(translations)):\n",
    "                if translations[i]['filename'].lower() == filename.lower():\n",
    "                    return translations[i]\n",
    "    else:\n",
    "        logger.error(\"Metadata is not Dictionary Object.\")\n",
    "        \n",
    "def connect_db(params_dict):\n",
    "    \"\"\"\n",
    "    Connect to the PostgreSQL database server\n",
    "    returns connection instance.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        logger.debug('Connecting to the PostgreSQL Database...')\n",
    "        conn = psycopg2.connect(**params_dict)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "\n",
    "        logger.error(\"An error occurred while establishing connection to database.\")\n",
    "        \n",
    "        if DEBUG:\n",
    "            raise\n",
    "    \n",
    "    logger.debug('Connection Successful.')\n",
    "\n",
    "    return conn\n",
    "\n",
    "def split_met_aqm(df, met_columnnames, aqm_columnnames):\n",
    "    \"\"\"\n",
    "    splits combined dataframe into Meteorological and Air quality\n",
    "    dataframes\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    met_columns, aqm_columns = [], []\n",
    "\n",
    "    for column in cols:\n",
    "        if column in met_columnnames:\n",
    "            met_columns.append(column)\n",
    "        if column in aqm_columnnames:\n",
    "            aqm_columns.append(column)\n",
    "\n",
    "    return df[met_columns], df[aqm_columns]\n",
    "\n",
    "def predelete_records(conn, min_datetime, max_datetime, tablename, station_id):\n",
    "    \"\"\"\n",
    "    predeletes the existing records from database based on\n",
    "    min_datetime and max_datetime of calculated hourly average\n",
    "    records.\n",
    "    \"\"\"\n",
    "    conn = connect_db(params_dict)\n",
    "    if conn is not None:        \n",
    "        query = \"DELETE FROM \" + \"core_data.\" + tablename + \" WHERE station=\" + station_id + \" AND datetime>=\" + \"'\" + min_datetime + \"'\" + \" AND datetime<=\" + \"'\" + max_datetime + \"'\"\n",
    "        cur = conn.cursor()    \n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "\n",
    "            count = cur.rowcount\n",
    "            logger.info(f\"{count} records deleted.\")\n",
    "\n",
    "        except (Exception, psycopg2.DatabaseError) as error:            \n",
    "            logger.error(\"An error occurred while establishing connection to database.\")            \n",
    "            conn.rollback()         \n",
    "            if DEBUG:\n",
    "                raise\n",
    "        finally:\n",
    "            if cur:\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "                \n",
    "def get_min_max_datetime(df):\n",
    "    \"\"\"\n",
    "    returns minimum and maximum datetime \n",
    "    where `datetime` is time-series index.\n",
    "    \"\"\"\n",
    "    return df.index.min(), df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = load_metadata('MetaData.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_batch_insertion(conn, min_datetime, max_datetime, df, fieldnames, tablename, station_id, page_size=100):\n",
    "    \"\"\"\n",
    "    using psycopg2.extras.execute_batch() to insert the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    predelete_records(conn, str(min_datetime), str(max_datetime), str(tablename), str(station_id))\n",
    "    # batch execution\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    query = 'INSERT INTO ' + 'core_data.' + tablename + '(' + ','.join(fieldname for fieldname in fieldnames) + ') VALUES(' + ','.join(['%s'] * len(fieldnames)) + ');'\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_batch(cur, query, tuples, page_size)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        logger.error(\"An error occurred while establishing connection to database.\")\n",
    "        conn.rollback()\n",
    "        if DEBUG:\n",
    "            raise\n",
    "    finally:\n",
    "        logger.info(\"Successfully loaded all data to PostgreSQL\")\n",
    "        if cur:\n",
    "            cur.close()\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(filename, metadata, met_tablename='dwer_met', aqm_tablename='dwer_aqm'):\n",
    "    \n",
    "    # extract only filenames from the file_path\n",
    "    splitted_filenames = filename.split('_')\n",
    "    date, extension = splitted_filenames[-1].split('.')\n",
    "    filename_without_date = '_'.join(splitted_filenames[0:-1]) + '.' + extension\n",
    "    csv_file = filename_without_date.split('/')[-1]\n",
    "\n",
    "    # get the column names mapping from metadata\n",
    "    columnnames_tx = get_translation_from_filename(csv_file, metadata)\n",
    "\n",
    "    if not columnnames_tx:\n",
    "        logger.error(f\"Could not find translations of {csv_file}.\")\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # processing the dataframe\n",
    "    if 'Date_Time.1' in list(df.columns):\n",
    "        df = df.drop('Date_Time.1', axis=1)\n",
    "    \n",
    "    # rename columns as in MetaData\n",
    "    df.rename(columns=columnnames_tx, inplace=True)\n",
    "    \n",
    "    std_columns = list(columnnames_tx.values())\n",
    "    renamed_columns = list(df.columns)\n",
    "    \n",
    "    # drop duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    \n",
    "    # drop columns if not present in Metadata\n",
    "    for value in renamed_columns:\n",
    "        if value not in std_columns:\n",
    "            logger.info(f\"Dropping {value} column\")\n",
    "            df = df.drop(value, axis=1)\n",
    "            logger.info(f\"Successfully dropped {value} column\")\n",
    "            logger.error(f\"{value} is missing in {csv_file}\")\n",
    "    \n",
    "    # generate column aggregate mapping for a given file\n",
    "    col_agg_mappings = { key: column_agg_mapping[key] for key in list(columnnames_tx.values())[1:]}\n",
    "    \n",
    "    # set `datetime` as the index of df\n",
    "    data_series = pd.notnull(df[\"datetime\"])\n",
    "    df['datetime'] = df['datetime'][data_series].apply(lambda x: datetime.strptime(x, '%d%m%Y %H%M'))\n",
    "    indexes = pd.DatetimeIndex(df['datetime'])\n",
    "    df = df.set_index(indexes)\n",
    "    \n",
    "    # generate hourly average records\n",
    "    try:\n",
    "        df_hourly = df.resample(SAMPLING_FREQUENCY).agg(col_agg_mappings)   \n",
    "        \n",
    "    except pd.core.base.SpecificationError:\n",
    "        if DEBUG:\n",
    "            raise\n",
    "    \n",
    "    else:\n",
    "        if 'rain' in list(df_hourly.columns):\n",
    "            df_hourly.loc[df_hourly['rain'] == 0, 'rain'] = None\n",
    "            \n",
    "        # drop records if all the records are null\n",
    "        df_hourly = df_hourly.dropna(how='all', axis=0)\n",
    "\n",
    "        # add sampled datetime index to new column    \n",
    "        df_hourly.insert(0, 'datetime', df_hourly.index)\n",
    "\n",
    "        # replace NaN to Null(None)\n",
    "        df_hourly = df_hourly.where(pd.notnull(df_hourly), None)\n",
    "\n",
    "        min_datetime, max_datetime = get_min_max_datetime(df_hourly)\n",
    "\n",
    "        # log minimum and maximum datetime, total records\n",
    "        logger.info(f\"MIN DATETIME in {csv_file}: {min_datetime}\")\n",
    "        logger.info(f\"MAX DATETIME in {csv_file}: {max_datetime}\")\n",
    "        logger.info(f\"Total records in {csv_file}: {df_hourly.size}\")\n",
    "\n",
    "        # get metadata of the station\n",
    "        station_metadata = get_metadata_from_filename(csv_file, metadata)\n",
    "        \n",
    "        station_id = station_metadata['stationid']\n",
    "        df_hourly['datetime'] = df_hourly['datetime'].apply(lambda x: x.to_pydatetime())\n",
    "        df_hourly.insert(1, 'station', int(station_id))\n",
    "        df_hourly.insert(2, 'interval_length', 60)\n",
    "\n",
    "        fieldnames = list(station_metadata['columnnames'].values())\n",
    "        fieldnames.insert(1, 'station')\n",
    "        fieldnames.insert(2, 'interval_length')\n",
    "        \n",
    "        if station_metadata['filetype'].lower() == 'm':\n",
    "            logger.debug(f\"{csv_file} has Meteorological Data.\")\n",
    "            \n",
    "            conn = connect_db(params_dict)\n",
    "            \n",
    "            execute_batch_insertion(\n",
    "                conn, \n",
    "                min_datetime, max_datetime, \n",
    "                df_hourly.round(4), fieldnames, met_tablename, \n",
    "                station_id,\n",
    "                50\n",
    "            )\n",
    "        elif station_metadata['filetype'].lower() == 'a':\n",
    "            logger.debug(f\"{csv_file} has Air Quality Data.\")\n",
    "            \n",
    "            conn = connect_db(params_dict)\n",
    "            \n",
    "            execute_batch_insertion(\n",
    "                conn,\n",
    "                min_datetime, max_datetime,\n",
    "                df_hourly.round(4), fieldnames, aqm_tablename,\n",
    "                station_id,\n",
    "                50\n",
    "            )\n",
    "        elif station_metadata['filetype'].lower() == 'ma':\n",
    "            logger.debug(f\"{csv_file} has both Meteorological and Air Quality Data.\")\n",
    "\n",
    "            # split the combined dataframe\n",
    "            df_hourly_met, df_hourly_aqm = split_met_aqm(df_hourly.round(4), met_columnnames, aqm_columnnames)\n",
    "            \n",
    "            min_datetime_met, max_datetime_met = get_min_max_datetime(df_hourly_met)\n",
    "            min_datetime_aqm, max_datetime_aqm = get_min_max_datetime(df_hourly_aqm)\n",
    "            \n",
    "            met_fieldnames = list(df_hourly_met.columns)\n",
    "            aqm_fieldnames = list(df_hourly_aqm.columns)\n",
    "            \n",
    "            conn = connect_db(params_dict)\n",
    "            \n",
    "            execute_batch_insertion(\n",
    "                conn,\n",
    "                min_datetime_met, max_datetime_met,\n",
    "                df_hourly_met.round(4), met_fieldnames, met_tablename,\n",
    "                station_id,\n",
    "                1\n",
    "            )\n",
    "            \n",
    "            conn = connect_db(params_dict)\n",
    "            \n",
    "            execute_batch_insertion(\n",
    "                conn,\n",
    "                min_datetime_aqm, max_datetime_aqm,\n",
    "                df_hourly_aqm.round(4), aqm_fieldnames, aqm_tablename,\n",
    "                station_id,\n",
    "                1\n",
    "            )            \n",
    "        else:\n",
    "            logger.error(\"Filetype Mismatched!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_db('Collie_Raw_Data_20200719.csv', metadata, 'dwer_met', 'dwer_aqm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    filename = event['Records'][0]['s3']['object']['key']\n",
    "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    \n",
    "    csv_file = filename.split('/')[-1]\n",
    "    file_path = '/tmp/{}'.format(csv_file)\n",
    "    \n",
    "    try:\n",
    "        s3_client.download_file(bucket, filename, file_path)\n",
    "        logger.debug(f\"Successfully downloaded {csv_file} from s3\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            logger.error(\"The object does not exist\")\n",
    "    else:\n",
    "        logger.debug(f\"Started loading {csv_file} data\")\n",
    "        \n",
    "        load_to_db(file_path, metadata, 'dwer_met', 'dwer_aqm')\n",
    "    finally:\n",
    "        logger.info(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
